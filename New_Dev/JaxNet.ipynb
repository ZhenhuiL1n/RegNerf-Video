{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Any, Callable\n",
    "\n",
    "from flax import linen as nn\n",
    "import gin\n",
    "from internal import mip, utils  # pylint: disable=g-multiple-import\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from flax.core import freeze, unfreeze\n",
    "from typing import Any, Callable, Sequence\n",
    "from jax import lax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        bias: (5,),\n",
       "        kernel: (10, 5),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Dense(features=5)\n",
    "\n",
    "\n",
    "rng = random.PRNGKey(20200823)\n",
    "# key1, key2 = random.split(rng)\n",
    "x = random.normal(rng, (10,)) # Dummy input data\n",
    "params = model.init(rng, x) # Initialization call\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params) # Checking output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  FrozenDict is immutable.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    params['new_key'] = jnp.ones((2,2))\n",
    "except ValueError as e:\n",
    "    print(\"Error: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.18553361, -0.1401707 , -0.3689404 , -0.11251645,\n",
       "             -2.8037481 ], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (20, 10) ; y shape: (20, 5)\n"
     ]
    }
   ],
   "source": [
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "# Generate random ground truth W and b.\n",
    "key = random.PRNGKey(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "# Store the parameters in a FrozenDict pytree.\n",
    "true_params = freeze({'params': {'bias': b, 'kernel': W}})\n",
    "\n",
    "# Generate samples with additional noise.\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise,(n_samples, y_dim))\n",
    "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as JAX version but using model.apply().\n",
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched):\n",
    "  # Define the squared loss for a single pair (x,y)\n",
    "  # loss function here, personal defined\n",
    "  def squared_error(x, y):\n",
    "    pred = model.apply(params, x)\n",
    "    return jnp.inner(y-pred, y-pred) / 2.0\n",
    "  # Vectorize the previous to compute the average of the loss on all samples.\n",
    "  return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  0.023639798\n",
      "Loss step 0:  31.742035\n",
      "Loss step 10:  0.5414006\n",
      "Loss step 20:  0.12205205\n",
      "Loss step 30:  0.040281765\n",
      "Loss step 40:  0.019850202\n",
      "Loss step 50:  0.014116081\n",
      "Loss step 60:  0.0123820845\n",
      "Loss step 70:  0.011833681\n",
      "Loss step 80:  0.011655833\n",
      "Loss step 90:  0.011597362\n",
      "Loss step 100:  0.011578011\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3  # Gradient step size.\n",
    "print('Loss for \"true\" W,b: ', mse(true_params, x_samples, y_samples))\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "@jax.jit\n",
    "def update_params(params, learning_rate, grads):\n",
    "  params = jax.tree_util.tree_map(\n",
    "      lambda p, g: p - learning_rate * g, params, grads)\n",
    "  return params\n",
    "\n",
    "for i in range(101):\n",
    "  # Perform one gradient update.\n",
    "  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "  params = update_params(params, learning_rate, grads)\n",
    "  if i % 10 == 0:\n",
    "    print(f'Loss step {i}: ', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0:  0.011577001\n",
      "Loss step 10:  0.24471128\n",
      "Loss step 20:  0.072736286\n",
      "Loss step 30:  0.039242655\n",
      "Loss step 40:  0.023648137\n",
      "Loss step 50:  0.01604911\n",
      "Loss step 60:  0.012986958\n",
      "Loss step 70:  0.012020969\n",
      "Loss step 80:  0.01174228\n",
      "Loss step 90:  0.011646231\n",
      "Loss step 100:  0.011585994\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "tx = optax.adam(learning_rate=learning_rate)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "  updates, opt_state = tx.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  if i % 10 == 0:\n",
    "    print('Loss step {}: '.format(i), loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
      "output:\n",
      " [[ 4.2292755e-02 -4.3807048e-02  2.9323749e-02  6.5492429e-03\n",
      "  -1.7147159e-02]\n",
      " [ 1.2967803e-01 -1.4551786e-01  9.4432145e-02  1.2521381e-02\n",
      "  -4.5417286e-02]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [ 9.3024079e-04  2.7864418e-05  2.4478836e-04  8.1344321e-04\n",
      "  -1.0110774e-03]]\n"
     ]
    }
   ],
   "source": [
    "class ExplicitMLP(nn.Module):\n",
    "  features: Sequence[int]\n",
    "\n",
    "  def setup(self):\n",
    "    # we automatically know what to do with lists, dicts of submodules\n",
    "    self.layers = [nn.Dense(feat) for feat in self.features]\n",
    "    # for single submodules, we would just write:\n",
    "    # self.layer1 = nn.Dense(feat1)\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    x = inputs\n",
    "    for i, lyr in enumerate(self.layers):\n",
    "      x = lyr(x)\n",
    "      if i != len(self.layers) - 1:\n",
    "        x = nn.relu(x)\n",
    "    return x\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = ExplicitMLP(features=[3,4,5])\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(params)))\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
      "output:\n",
      " [[ 4.2292755e-02 -4.3807048e-02  2.9323749e-02  6.5492429e-03\n",
      "  -1.7147159e-02]\n",
      " [ 1.2967803e-01 -1.4551786e-01  9.4432145e-02  1.2521381e-02\n",
      "  -4.5417286e-02]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [ 9.3024079e-04  2.7864418e-05  2.4478836e-04  8.1344321e-04\n",
      "  -1.0110774e-03]]\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "  features: Sequence[int]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    x = inputs\n",
    "    for i, feat in enumerate(self.features):\n",
    "      x = nn.Dense(feat, name=f'layers_{i}')(x)\n",
    "      if i != len(self.features) - 1:\n",
    "        x = nn.relu(x)\n",
    "      # providing a name is optional though!\n",
    "      # the default autonames would be \"Dense_0\", \"Dense_1\", ...\n",
    "    return x\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = SimpleMLP(features=[3,4,5])\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(params)))\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'flax.linen.initializers' has no attribute 'zeros_init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-7037169f3036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSimpleDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mkernel_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlecun_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mbias_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-7037169f3036>\u001b[0m in \u001b[0;36mSimpleDense\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mkernel_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlecun_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mbias_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'flax.linen.initializers' has no attribute 'zeros_init'"
     ]
    }
   ],
   "source": [
    "class SimpleDense(nn.Module):\n",
    "  features: int\n",
    "  kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "  bias_init: Callable = nn.initializers.zeros_init()\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    kernel = self.param('kernel',\n",
    "                        self.kernel_init, # Initialization function\n",
    "                        (inputs.shape[-1], self.features))  # shape info.\n",
    "    y = lax.dot_general(inputs, kernel,\n",
    "                        (((inputs.ndim - 1,), (0,)), ((), ())),) # TODO Why not jnp.dot?\n",
    "    bias = self.param('bias', self.bias_init, (self.features,))\n",
    "    y = y + bias\n",
    "    return y\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = SimpleDense(features=3)\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameters:\\n', params)\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "def multiply(x, y):\n",
    "    return x * y\n",
    "\n",
    "# Create a new function using partial application\n",
    "double = functools.partial(multiply, y=2)\n",
    "\n",
    "# Call the new function\n",
    "result = double(5)\n",
    "\n",
    "print(result)  # Output: 10\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
