{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from os import path\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "from internal import math, utils  # pylint: disable=g-multiple-import\n",
    "import jax\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cam_1.mp4', 'cam_2.mp4', 'cam_3.mp4', 'cam_4.mp4', 'cam_5.mp4', 'cam_6.mp4', 'cam_7.mp4', 'cam_8.mp4']\n"
     ]
    }
   ],
   "source": [
    "Path_to_videos = '/media/pleasework/Storage/Nerf_Datasets/Datasets/surrey/02_dancer'\n",
    "\n",
    "\n",
    "# only save the file with end with .mp4\n",
    "names = os.listdir(Path_to_videos)\n",
    "\n",
    "for name in names:\n",
    "    if not name.endswith('.mp4'):\n",
    "        names.remove(name)\n",
    "\n",
    "names.sort()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = r'/media/pleasework/Storage/regnerf/data/pop-sparse/metadata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(img, factor, patch_size=-1, mode=cv2.INTER_AREA):\n",
    "  \"\"\"Area downsample img (factor must evenly divide img height and width).\"\"\"\n",
    "  sh = img.shape\n",
    "  max_fn = lambda x: max(x, patch_size)\n",
    "  out_shape = (max_fn(sh[1] // factor), max_fn(sh[0] // factor))\n",
    "  img = cv2.resize(img, out_shape, mode)\n",
    "  return img\n",
    "\n",
    "def sample_recon_scale(image_list, dist='uniform_scale'):\n",
    "  \"\"\"Samples a scale factor for the reconstruction loss.\"\"\"\n",
    "  if dist == 'uniform_scale':\n",
    "    idx = np.random.randint(len(image_list))\n",
    "  elif dist == 'uniform_size':\n",
    "    n_img = np.array([i.shape[0] for i in image_list], dtype=np.float32)\n",
    "    probs = n_img / np.sum(n_img)\n",
    "    idx = np.random.choice(np.arange(len(image_list)), size=(), p=probs)\n",
    "  return idx\n",
    "\n",
    "def anneal_nearfar(d, it, near_final, far_final,\n",
    "                   n_steps=2000, init_perc=0.2, mid_perc=0.5):\n",
    "  \"\"\"Anneals near and far plane.\"\"\"\n",
    "  mid = near_final + mid_perc * (far_final - near_final)\n",
    "\n",
    "  near_init = mid + init_perc * (near_final - mid)\n",
    "  far_init = mid + init_perc * (far_final - mid)\n",
    "\n",
    "  weight = min(it * 1.0 / n_steps, 1.0)\n",
    "\n",
    "  near_i = near_init + weight * (near_final - near_init)\n",
    "  far_i = far_init + weight * (far_final - far_init)\n",
    "\n",
    "  out_dict = {}\n",
    "  for (k, v) in d.items():\n",
    "    if 'rays' in k and isinstance(v, utils.Rays):\n",
    "      ones = np.ones_like(v.origins[Ellipsis, :1])\n",
    "      rays_out = utils.Rays(\n",
    "          origins=v.origins, directions=v.directions,\n",
    "          viewdirs=v.viewdirs, radii=v.radii,\n",
    "          lossmult=v.lossmult, near=ones*near_i, far=ones*far_i)\n",
    "      out_dict[k] = rays_out\n",
    "    else:\n",
    "      out_dict[k] = v\n",
    "  return out_dict\n",
    "\n",
    "\n",
    "def subsample_patches(images, patch_size, batch_size, batching='all_images'):\n",
    "  \"\"\"Subsamples patches.\"\"\"\n",
    "  n_patches = batch_size // (patch_size ** 2)\n",
    "\n",
    "  scale = np.random.randint(0, len(images))\n",
    "  images = images[scale]\n",
    "\n",
    "  if isinstance(images, np.ndarray):\n",
    "    shape = images.shape\n",
    "  else:\n",
    "    shape = images.origins.shape\n",
    "\n",
    "  # Sample images\n",
    "  if batching == 'all_images':\n",
    "    idx_img = np.random.randint(0, shape[0], size=(n_patches, 1))\n",
    "  elif batching == 'single_image':\n",
    "    idx_img = np.random.randint(0, shape[0])\n",
    "    idx_img = np.full((n_patches, 1), idx_img, dtype=int)\n",
    "  else:\n",
    "    raise ValueError('Not supported batching type!')\n",
    "\n",
    "  # Sample start locations\n",
    "  x0 = np.random.randint(0, shape[2] - patch_size + 1, size=(n_patches, 1, 1))\n",
    "  y0 = np.random.randint(0, shape[1] - patch_size + 1, size=(n_patches, 1, 1))\n",
    "  xy0 = np.concatenate([x0, y0], axis=-1)\n",
    "  patch_idx = xy0 + np.stack(\n",
    "      np.meshgrid(np.arange(patch_size), np.arange(patch_size), indexing='xy'),\n",
    "      axis=-1).reshape(1, -1, 2)\n",
    "\n",
    "  # Subsample images\n",
    "  if isinstance(images, np.ndarray):\n",
    "    out = images[idx_img, patch_idx[Ellipsis, 1], patch_idx[Ellipsis, 0]].reshape(-1, 3)\n",
    "  else:\n",
    "    out = utils.dataclass_map(\n",
    "        lambda x: x[idx_img, patch_idx[Ellipsis, 1], patch_idx[Ellipsis, 0]].reshape(  # pylint: disable=g-long-lambda\n",
    "            -1, x.shape[-1]), images)\n",
    "  return out, np.ones((n_patches, 1), dtype=np.float32) * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(threading.Thread):\n",
    "  \"\"\"Dataset Base Class.\"\"\"\n",
    "\n",
    "  def __init__(self, split, data_dir, config):\n",
    "    super(Dataset, self).__init__()\n",
    "    self.queue = queue.Queue(3)  # Set prefetch buffer to 3 batches.\n",
    "    self.daemon = True\n",
    "    self.use_tiffs = config.use_tiffs\n",
    "    self.load_disps = config.compute_disp_metrics\n",
    "    self.load_normals = config.compute_normal_metrics\n",
    "    self.load_random_rays = config.load_random_rays\n",
    "    self.load_random_fullimage_rays = config.dietnerf_loss_mult != 0.0\n",
    "    self.load_masks = ((config.dataset_loader == 'dtu') and (split == 'test')\n",
    "                       and (not config.dtu_no_mask_eval)\n",
    "                       and (not config.render_path))\n",
    "\n",
    "    self.split = split\n",
    "    if config.dataset_loader == 'dtu':\n",
    "      self.data_base_dir = data_dir\n",
    "      data_dir = os.path.join(data_dir, config.dtu_scan)\n",
    "    elif config.dataset_loader == 'llff':\n",
    "      self.data_base_dir = data_dir\n",
    "      data_dir = os.path.join(data_dir, config.llff_scan)\n",
    "    elif config.dataset_loader == 'blender':\n",
    "      self.data_base_dir = data_dir\n",
    "      data_dir = os.path.join(data_dir, config.blender_scene)\n",
    "    self.data_dir = data_dir\n",
    "    self.near = config.near\n",
    "    self.far = config.far\n",
    "    self.near_origin = config.near_origin\n",
    "    self.anneal_nearfar = config.anneal_nearfar\n",
    "    self.anneal_nearfar_steps = config.anneal_nearfar_steps\n",
    "    self.anneal_nearfar_perc = config.anneal_nearfar_perc\n",
    "    self.anneal_mid_perc = config.anneal_mid_perc\n",
    "    self.sample_reconscale_dist = config.sample_reconscale_dist\n",
    "\n",
    "    if split == 'train':\n",
    "      self._train_init(config)\n",
    "    elif split == 'test' or split == 'path':\n",
    "      self._test_init(config)\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          f'`split` should be \\'train\\' or \\'test\\', but is \\'{split}\\'.')\n",
    "    self.batch_size = config.batch_size // jax.host_count()\n",
    "    self.batch_size_random = config.batch_size_random // jax.host_count()\n",
    "    print('Using following batch size', self.batch_size)\n",
    "    self.patch_size = config.patch_size\n",
    "    self.batching = config.batching\n",
    "    self.batching_random = config.batching_random\n",
    "    self.render_path = config.render_path\n",
    "    self.render_train = config.render_train\n",
    "    self.start()\n",
    "    \n",
    "\n",
    "  def __iter__(self):\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    \"\"\"Get the next training batch or test example.\n",
    "\n",
    "    Returns:\n",
    "      batch: dict, has 'rgb' and 'rays'.\n",
    "    \"\"\"\n",
    "    x = self.queue.get()\n",
    "    if self.split == 'train':\n",
    "      return utils.shard(x)\n",
    "    else:\n",
    "      return utils.to_device(x)\n",
    "\n",
    "  def peek(self):\n",
    "    \"\"\"Peek at the next training batch or test example without dequeuing it.\n",
    "\n",
    "    Returns:\n",
    "      batch: dict, has 'rgb' and 'rays'.\n",
    "    \"\"\"\n",
    "    x = self.queue.queue[0].copy()  # Make a copy of the front of the queue.\n",
    "    if self.split == 'train':\n",
    "      return utils.shard(x)\n",
    "    else:\n",
    "      return utils.to_device(x)\n",
    "\n",
    "  def run(self):\n",
    "    if self.split == 'train':\n",
    "      next_func = self._next_train\n",
    "    else:\n",
    "      next_func = self._next_test\n",
    "    while True:\n",
    "      self.queue.put(next_func())\n",
    "      print(\"Queue size\", self.queue.qsize())\n",
    "\n",
    "  @property\n",
    "  def size(self):\n",
    "    return self.n_examples\n",
    "\n",
    "  def _train_init(self, config):\n",
    "    \"\"\"Initialize training.\"\"\"\n",
    "    self._load_renderings(config)\n",
    "    self._generate_downsampled_images(config)\n",
    "    self._generate_rays(config)\n",
    "    self._generate_downsampled_rays(config)\n",
    "\n",
    "    # Generate more rays / image patches for unobserved-view-based losses.\n",
    "    if self.load_random_rays:\n",
    "      self._generate_random_rays(config)\n",
    "    if self.load_random_fullimage_rays:\n",
    "      self._generate_random_fullimage_rays(config)\n",
    "      self._load_renderings_featloss(config)\n",
    "\n",
    "    self.it = 0\n",
    "    self.images_noreshape = self.images[0]\n",
    "\n",
    "    if config.batching == 'all_images':\n",
    "      # flatten the ray and image dimension together.\n",
    "      self.images = [i.reshape(-1, 3) for i in self.images]\n",
    "      if self.load_disps:\n",
    "        self.disp_images = self.disp_images.flatten()\n",
    "      if self.load_normals:\n",
    "        self.normal_images = self.normal_images.reshape([-1, 3])\n",
    "\n",
    "      self.ray_noreshape = [self.rays]\n",
    "      self.rays = [utils.dataclass_map(lambda r: r.reshape(  # pylint: disable=g-long-lambda\n",
    "          [-1, r.shape[-1]]), i) for (i, res) in zip(\n",
    "              self.rays, self.resolutions)]\n",
    "\n",
    "  \n",
    "    elif config.batching == 'single_image':\n",
    "      print(\"image shape:\",self.images_noreshape.shape)\n",
    "      self.images = [i.reshape(\n",
    "          [-1, r, 3]) for (i, r) in zip(self.images, self.resolutions)]\n",
    "      print(\"shape after operation:\",self.images[0].shape)\n",
    "      if self.load_disps:\n",
    "        self.disp_images = self.disp_images.reshape([-1, self.resolution])\n",
    "      if self.load_normals:\n",
    "        self.normal_images = self.normal_images.reshape(\n",
    "            [-1, self.resolution, 3])\n",
    "      \n",
    "      self.ray_noreshape = [self.rays]\n",
    "      # print(\"rays:\",self.rays)    \n",
    "      self.rays = [utils.dataclass_map(lambda r: r.reshape(  # pylint: disable=g-long-lambda\n",
    "          [-1, res, r.shape[-1]]), i) for (i, res) in  # pylint: disable=cell-var-from-loop\n",
    "                   zip(self.rays, self.resolutions)]\n",
    "      print(\"self.resolutions:\", self.resolutions)\n",
    "      # print(\"rays:\",self.rays)   \n",
    "    else:\n",
    "      raise NotImplementedError(\n",
    "          f'{config.batching} batching strategy is not implemented.')\n",
    "    # print(\"rays:\",self.rays)\n",
    "\n",
    "  def _test_init(self, config):\n",
    "    self._load_renderings(config)\n",
    "    if self.load_masks:\n",
    "      self._load_masks(config)\n",
    "    self._generate_rays(config)\n",
    "    self.it = 0\n",
    "\n",
    "  def _next_train(self):\n",
    "    \"\"\"Sample next training batch.\"\"\"\n",
    "\n",
    "    self.it = self.it + 1\n",
    "    print(\"self.it:\",self.it)\n",
    "    return_dict = {}\n",
    "    if self.batching == 'all_images':\n",
    "      # sample scale\n",
    "      idxs = sample_recon_scale(self.images, self.sample_reconscale_dist)\n",
    "      ray_indices = np.random.randint(0, self.rays[idxs].origins.shape[0],\n",
    "                                      (self.batch_size,))\n",
    "      return_dict['rgb'] = self.images[idxs][ray_indices]\n",
    "      return_dict['rays'] = utils.dataclass_map(lambda r: r[ray_indices],\n",
    "                                                self.rays[idxs])\n",
    "      if self.load_disps:\n",
    "        return_dict['disps'] = self.disp_images[ray_indices]\n",
    "      if self.load_normals:\n",
    "        return_dict['normals'] = self.normal_images[ray_indices]\n",
    "\n",
    "    elif self.batching == 'single_image':\n",
    "      idxs = sample_recon_scale(self.images, self.sample_reconscale_dist)\n",
    "      idxs = 0\n",
    "      print(\"idxs:\",idxs)\n",
    "      image_index = np.random.randint(0, self.n_examples, ())\n",
    "      ray_indices = np.random.randint(0, self.rays[idxs].origins[0].shape[0],\n",
    "                                      (self.batch_size,))\n",
    "      print(\"ray_indices:\", ray_indices)\n",
    "      print(\"image index:\", image_index)\n",
    "      return_dict['rgb'] = self.images[idxs][image_index][ray_indices]\n",
    "      return_dict['rays'] = utils.dataclass_map(\n",
    "          lambda r: r[image_index][ray_indices], self.rays[idxs])\n",
    "      if self.load_disps:\n",
    "        return_dict['disps'] = self.disp_images[image_index][ray_indices]\n",
    "      if self.load_normals:\n",
    "        return_dict['normals'] = self.normal_images[image_index][ray_indices]\n",
    "    else:\n",
    "      raise NotImplementedError(\n",
    "          f'{self.batching} batching strategy is not implemented.')\n",
    "\n",
    "    if self.load_random_rays:\n",
    "      return_dict['rays_random'], return_dict['rays_random_scale'] = (\n",
    "          subsample_patches(self.random_rays, self.patch_size,\n",
    "                            self.batch_size_random,\n",
    "                            batching=self.batching_random))\n",
    "      return_dict['rays_random2'], return_dict['rays_random2_scale'] = (\n",
    "          subsample_patches(\n",
    "              self.random_rays, self.patch_size, self.batch_size_random,\n",
    "              batching=self.batching_random))\n",
    "    if self.load_random_fullimage_rays:\n",
    "      idx_img = np.random.randint(self.random_fullimage_rays.origins.shape[0])\n",
    "      return_dict['rays_feat'] = utils.dataclass_map(\n",
    "          lambda x: x[idx_img].reshape(-1, x.shape[-1]),\n",
    "          self.random_fullimage_rays)\n",
    "      idx_img = np.random.randint(self.images_feat.shape[0])\n",
    "      return_dict['image_feat'] = self.images_feat[idx_img].reshape(-1, 3)\n",
    "\n",
    "    if self.anneal_nearfar:\n",
    "      return_dict = anneal_nearfar(return_dict, self.it, self.near, self.far,\n",
    "                                   self.anneal_nearfar_steps,\n",
    "                                   self.anneal_nearfar_perc,\n",
    "                                   self.anneal_mid_perc)\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "  def _next_test(self):\n",
    "    \"\"\"Sample next test example.\"\"\"\n",
    "\n",
    "    return_dict = {}\n",
    "\n",
    "    idx = self.it\n",
    "    self.it = (self.it + 1) % self.n_examples\n",
    "\n",
    "    if self.render_path:\n",
    "      return_dict['rays'] = utils.dataclass_map(lambda r: r[idx],\n",
    "                                                self.render_rays)\n",
    "    else:\n",
    "      return_dict['rgb'] = self.images[idx]\n",
    "      return_dict['rays'] = utils.dataclass_map(lambda r: r[idx], self.rays)\n",
    "\n",
    "    if self.load_masks:\n",
    "      return_dict['mask'] = self.masks[idx]\n",
    "    if self.load_disps:\n",
    "      return_dict['disps'] = self.disp_images[idx]\n",
    "    if self.load_normals:\n",
    "      return_dict['normals'] = self.normal_images[idx]\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "  def _generate_rays(self, config):\n",
    "\n",
    "    print(\"using the generate rays from dataset classsssssssss\")\n",
    "    \"\"\"Generating rays for all images.\"\"\"\n",
    "    del config  # Unused.\n",
    "    x, y = np.meshgrid(  # pylint: disable=unbalanced-tuple-unpacking\n",
    "        np.arange(self.width, dtype=np.float32),  # X-Axis (columns)\n",
    "        np.arange(self.height, dtype=np.float32),  # Y-Axis (rows)\n",
    "        indexing='xy')\n",
    "    camera_dirs = np.stack(\n",
    "        [(x - self.width * 0.5 + 0.5) / self.focal,\n",
    "         -(y - self.height * 0.5 + 0.5) / self.focal, -np.ones_like(x)],\n",
    "        axis=-1)\n",
    "    directions = ((camera_dirs[None, Ellipsis, None, :] *\n",
    "                   self.camtoworlds[:, None, None, :3, :3]).sum(axis=-1))\n",
    "    origins = np.broadcast_to(self.camtoworlds[:, None, None, :3, -1],\n",
    "                              directions.shape)\n",
    "    viewdirs = directions / np.linalg.norm(directions, axis=-1, keepdims=True)\n",
    "\n",
    "    # Distance from each unit-norm direction vector to its x-axis neighbor.\n",
    "    dx = np.sqrt(\n",
    "        np.sum((directions[:, :-1, :, :] - directions[:, 1:, :, :])**2, -1))\n",
    "    dx = np.concatenate([dx, dx[:, -2:-1, :]], axis=1)\n",
    "    # Cut the distance in half, multiply it to match the variance of a uniform\n",
    "    # distribution the size of a pixel (1/12, see paper).\n",
    "    radii = dx[Ellipsis, None] * 2 / np.sqrt(12)\n",
    "\n",
    "    ones = np.ones_like(origins[Ellipsis, :1])\n",
    "    self.rays = utils.Rays(\n",
    "        origins=origins,\n",
    "        directions=directions,\n",
    "        viewdirs=viewdirs,\n",
    "        lossmult=ones,\n",
    "        radii=radii,\n",
    "        near=ones * self.near,\n",
    "        far=ones * self.far)\n",
    "    self.render_rays = self.rays\n",
    "\n",
    "  def _generate_random_poses(self, config):\n",
    "    \"\"\"Generates random poses.\"\"\"\n",
    "    if config.random_pose_type == 'allposes':\n",
    "      random_poses = list(self.camtoworlds_all)\n",
    "    elif config.random_pose_type == 'renderpath':\n",
    "      def sample_on_sphere(n_samples, only_upper=True, radius=4.03112885717555):\n",
    "        p = np.random.randn(n_samples, 3)\n",
    "        if only_upper:\n",
    "          p[:, -1] = abs(p[:, -1])\n",
    "        p = p / np.linalg.norm(p, axis=-1, keepdims=True) * radius\n",
    "        return p\n",
    "\n",
    "      def create_look_at(eye, target=np.array([0, 0, 0]),\n",
    "                         up=np.array([0, 0, 1]), dtype=np.float32):\n",
    "        \"\"\"Creates lookat matrix.\"\"\"\n",
    "        eye = eye.reshape(-1, 3).astype(dtype)\n",
    "        target = target.reshape(-1, 3).astype(dtype)\n",
    "        up = up.reshape(-1, 3).astype(dtype)\n",
    "\n",
    "        def normalize_vec(x, eps=1e-9):\n",
    "          return x / (np.linalg.norm(x, axis=-1, keepdims=True) + eps)\n",
    "\n",
    "        forward = normalize_vec(target - eye)\n",
    "        side = normalize_vec(np.cross(forward, up))\n",
    "        up = normalize_vec(np.cross(side, forward))\n",
    "\n",
    "        up = up * np.array([1., 1., 1.]).reshape(-1, 3)\n",
    "        forward = forward * np.array([-1., -1., -1.]).reshape(-1, 3)\n",
    "\n",
    "        rot = np.stack([side, up, forward], axis=-1).astype(dtype)\n",
    "        return rot\n",
    "\n",
    "      origins = sample_on_sphere(config.n_random_poses)\n",
    "      rotations = create_look_at(origins)\n",
    "      random_poses = np.concatenate([rotations, origins[:, :, None]], axis=-1)\n",
    "    else:\n",
    "      raise ValueError('Not supported random pose type.')\n",
    "    self.random_poses = np.stack(random_poses, axis=0)\n",
    "\n",
    "  def _generate_random_rays(self, config):\n",
    "    \"\"\"Generating rays for all images.\"\"\"\n",
    "    self._generate_random_poses(config)\n",
    "\n",
    "    random_rays = []\n",
    "    for sfactor in [2**i for i in range(config.random_scales_init,\n",
    "                                        config.random_scales)]:\n",
    "      w = self.width // sfactor\n",
    "      h = self.height // sfactor\n",
    "      f = self.focal / (sfactor * 1.0)\n",
    "      x, y = np.meshgrid(  # pylint: disable=unbalanced-tuple-unpacking\n",
    "          np.arange(w, dtype=np.float32),  # X-Axis (columns)\n",
    "          np.arange(h, dtype=np.float32),  # Y-Axis (rows)\n",
    "          indexing='xy')\n",
    "      camera_dirs = np.stack(\n",
    "          [(x - w * 0.5 + 0.5) / f,\n",
    "           -(y - h * 0.5 + 0.5) / f, -np.ones_like(x)],\n",
    "          axis=-1)\n",
    "      directions = ((camera_dirs[None, Ellipsis, None, :] *\n",
    "                     self.random_poses[:, None, None, :3, :3]).sum(axis=-1))\n",
    "      origins = np.broadcast_to(self.random_poses[:, None, None, :3, -1],\n",
    "                                directions.shape)\n",
    "      viewdirs = directions / np.linalg.norm(directions, axis=-1, keepdims=True)\n",
    "\n",
    "      # Distance from each unit-norm direction vector to its x-axis neighbor.\n",
    "      dx = np.sqrt(\n",
    "          np.sum((directions[:, :-1, :, :] - directions[:, 1:, :, :])**2, -1))\n",
    "      dx = np.concatenate([dx, dx[:, -2:-1, :]], axis=1)\n",
    "      # Cut the distance in half, multiply it to match the variance of a uniform\n",
    "      # distribution the size of a pixel (1/12, see paper).\n",
    "      radii = dx[Ellipsis, None] * 2 / np.sqrt(12)\n",
    "\n",
    "      ones = np.ones_like(origins[Ellipsis, :1])\n",
    "      rays = utils.Rays(\n",
    "          origins=origins,\n",
    "          directions=directions,\n",
    "          viewdirs=viewdirs,\n",
    "          radii=radii,\n",
    "          lossmult=ones,\n",
    "          near=ones * self.near,\n",
    "          far=ones * self.far)\n",
    "      random_rays.append(rays)\n",
    "    self.random_rays = random_rays\n",
    "\n",
    "  def _load_renderings_featloss(self, config):\n",
    "    \"\"\"Loades renderings for DietNeRF's feature loss.\"\"\"\n",
    "    images = self.images[0]\n",
    "    res = config.dietnerf_loss_resolution\n",
    "    images_feat = []\n",
    "    for img in images:\n",
    "      images_feat.append(cv2.resize(img, (res, res), cv2.INTER_AREA))\n",
    "    self.images_feat = np.stack(images_feat)\n",
    "\n",
    "  def _generate_random_fullimage_rays(self, config):\n",
    "    \"\"\"Generating random rays for full images.\"\"\"\n",
    "    self._generate_random_poses(config)\n",
    "\n",
    "    width = config.dietnerf_loss_resolution\n",
    "    height = config.dietnerf_loss_resolution\n",
    "    f = self.focal / (self.width * 1.0 / width)\n",
    "\n",
    "    x, y = np.meshgrid(  # pylint: disable=unbalanced-tuple-unpacking\n",
    "        np.arange(width, dtype=np.float32) + .5,\n",
    "        np.arange(height, dtype=np.float32) + .5,\n",
    "        indexing='xy')\n",
    "\n",
    "    camera_dirs = np.stack([(x - width * 0.5 + 0.5) / f,\n",
    "                            -(y - height * 0.5 + 0.5) / f,\n",
    "                            -np.ones_like(x)], axis=-1)\n",
    "    directions = ((camera_dirs[None, Ellipsis, None, :] *\n",
    "                   self.random_poses[:, None, None, :3, :3]).sum(axis=-1))\n",
    "    origins = np.broadcast_to(self.random_poses[:, None, None, :3, -1],\n",
    "                              directions.shape)\n",
    "    viewdirs = directions / np.linalg.norm(directions, axis=-1, keepdims=True)\n",
    "\n",
    "    # Distance from each unit-norm direction vector to its x-axis neighbor.\n",
    "    dx = np.sqrt(\n",
    "        np.sum((directions[:, :-1, :, :] - directions[:, 1:, :, :])**2, -1))\n",
    "    dx = np.concatenate([dx, dx[:, -2:-1, :]], axis=1)\n",
    "    # Cut the distance in half, multiply it to match the variance of a uniform\n",
    "    # distribution the size of a pixel (1/12, see paper).\n",
    "    radii = dx[Ellipsis, None] * 2 / np.sqrt(12)\n",
    "\n",
    "    ones = np.ones_like(origins[Ellipsis, :1])\n",
    "    self.random_fullimage_rays = utils.Rays(\n",
    "        origins=origins,\n",
    "        directions=directions,\n",
    "        viewdirs=viewdirs,\n",
    "        radii=radii,\n",
    "        lossmult=ones,\n",
    "        near=ones * self.near,\n",
    "        far=ones * self.far)\n",
    "\n",
    "  def _generate_downsampled_images(self, config):\n",
    "    \"\"\"Generating downsampled images.\"\"\"\n",
    "    images = []\n",
    "    resolutions = []\n",
    "    for sfactor in [2**i for i in range(config.recon_loss_scales)]:\n",
    "      print(\"single image shape:\",self.images[0].shape)\n",
    "      imgi = np.stack([downsample(i, sfactor) for i in self.images])\n",
    "      images.append(imgi)\n",
    "      # print(\"single image shape:\",imgi.shape)\n",
    "      resolutions.append(imgi.shape[1] * imgi.shape[2])\n",
    "\n",
    "    self.images = images\n",
    "    self.resolutions = resolutions\n",
    "\n",
    "  def _generate_downsampled_rays(self, config):\n",
    "    \"\"\"Generating downsampled images.\"\"\"\n",
    "    rays, height, width, focal = self.rays, self.height, self.width, self.focal\n",
    "    ray_list = [rays]\n",
    "    for sfactor in [2**i for i in range(1, config.recon_loss_scales)]:\n",
    "      self.height = height // sfactor\n",
    "      self.width = width // sfactor\n",
    "      self.focal = focal * 1.0 / sfactor\n",
    "      self._generate_rays(config)\n",
    "      ray_list.append(self.rays)\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "    self.focal = focal\n",
    "    self.rays = ray_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "# save each frame of each camera into the images, and also the camera poses will be listed correspondingly\n",
    "\n",
    "class multicam_video(Dataset):\n",
    "\n",
    "    def _load_renderings(self, config):\n",
    "        if config.render_path:\n",
    "            raise ValueError('render_path cannot be used for the Multicam dataset.')\n",
    "        with utils.open_file(path.join(self.data_dir, 'metadata.json'), 'r') as fp:\n",
    "            self.meta = json.load(fp)[self.split]\n",
    "            print(\"self.split:\", self.split)\n",
    "        self.meta = {k: np.array(self.meta[k]) for k in self.meta}\n",
    "        # Should now have ['pix2cam', 'cam2world', 'width', 'height'] in self.meta.\n",
    "        # print(self.meta)\n",
    "\n",
    "        # read the video sequence:\n",
    "        for i in range(len())\n",
    "        self.video = cv2.VideoCapture(path.join(self.data_dir, 'video.mp4'))\n",
    "\n",
    "\n",
    "        images = []\n",
    "\n",
    "        for fbase in self.meta['file_path']:\n",
    "            print('file path:', fbase)\n",
    "            fname = os.path.join(self.data_dir, fbase)\n",
    "            with utils.open_file(fname, 'rb') as imgin:\n",
    "                image = np.array(Image.open(imgin), dtype=np.float32) / 255.\n",
    "            if config.white_background:\n",
    "                image = image[Ellipsis, :3] * image[Ellipsis, -1:] + (1. - image[Ellipsis, -1:])\n",
    "            images.append(image[Ellipsis, :3])\n",
    "\n",
    "        self.images = np.stack(images, axis = 0)\n",
    "        print(\"image shape after stack:\", self.images.shape)\n",
    "        self.n_examples = len(self.images)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## So right now we should distribute the video sequences into the different frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'datasets' from 'internal' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-5b27bc237721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis\u001b[0m  \u001b[0;31m# pylint: disable=g-multiple-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'datasets' from 'internal' (unknown location)"
     ]
    }
   ],
   "source": [
    "# Main train used for testing the output of the multicam video sequences.\n",
    "\n",
    "import functools\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "import flax\n",
    "from flax.metrics import tensorboard\n",
    "from flax.training import checkpoints\n",
    "from internal import configs, datasets, math, models, utils, vis  # pylint: disable=g-multiple-import\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "def main():\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the camera poses,\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperreel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
